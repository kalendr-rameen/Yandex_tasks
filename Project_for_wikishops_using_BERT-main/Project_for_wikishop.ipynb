{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.LogisticRegression\" data-toc-modified-id=\"1.LogisticRegression-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>1.LogisticRegression</a></span></li><li><span><a href=\"#2.-SGDClassifier\" data-toc-modified-id=\"2.-SGDClassifier-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>2. SGDClassifier</a></span></li><li><span><a href=\"#3.-Catboost\" data-toc-modified-id=\"3.-Catboost-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>3. Catboost</a></span></li><li><span><a href=\"#4.-LGBMClassifier\" data-toc-modified-id=\"4.-LGBMClassifier-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>4. LGBMClassifier</a></span></li><li><span><a href=\"#5.-BERT\" data-toc-modified-id=\"5.-BERT-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>5. BERT</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» c BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymystem3 import Mystem\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import lightgbm as lgbm\n",
    "import catboost as cb\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import torch\n",
    "import torch\n",
    "import transformers \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ramen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lemm_sentences = []\n",
    "for i in range(df.shape[0]):\n",
    "    sentence = df.loc[i]['text']\n",
    "    word_list = re.sub(r'[^a-zA-Z ]', ' ', df.loc[i]['text']).split()\n",
    "    lemmatized_output =  ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    all_lemm_sentences.append(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemm'] = pd.Series(all_lemm_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,oth = train_test_split(df, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid, test = train_test_split(oth, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = pd.concat([train, valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train['lemm'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ramen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transformer = TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 2), lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = text_transformer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text = text_transformer.transform(test['lemm'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак данные готовы, теперь применим 4 модели(LogRegressing, Catboost, SGDClassifier, LGBMClassifier), и посмотрим кто же лучше всего может описать и предсказать по нашим данным<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='multinomial', random_state=17, n_jobs=-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=50.0, multi_class='multinomial', n_jobs=-1,\n",
       "                   random_state=17)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit(X_train_text, train['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.predict(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9606767977440075"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test['toxic'], logit.predict(X_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7850659359479363"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test['toxic'], logit.predict(X_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss = \"hinge\", penalty = \"l1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(penalty='l1')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_text, train['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6533066132264529"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test['toxic'], clf.predict(X_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cb.CatBoostClassifier(iterations=100, depth=10, loss_function='Logloss',random_seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom logger is already specified. Specify more than one logger at same time is not thread safe."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.5\n",
      "0:\tlearn: 0.3449195\ttotal: 19.1s\tremaining: 31m 31s\n",
      "1:\tlearn: 0.2516541\ttotal: 38.2s\tremaining: 31m 9s\n",
      "2:\tlearn: 0.2218982\ttotal: 57.2s\tremaining: 30m 49s\n",
      "3:\tlearn: 0.2084444\ttotal: 1m 16s\tremaining: 30m 33s\n",
      "4:\tlearn: 0.1983441\ttotal: 1m 35s\tremaining: 30m 15s\n",
      "5:\tlearn: 0.1912460\ttotal: 1m 54s\tremaining: 29m 55s\n",
      "6:\tlearn: 0.1857448\ttotal: 2m 13s\tremaining: 29m 37s\n",
      "7:\tlearn: 0.1805690\ttotal: 2m 32s\tremaining: 29m 19s\n",
      "8:\tlearn: 0.1762914\ttotal: 2m 52s\tremaining: 29m\n",
      "9:\tlearn: 0.1716879\ttotal: 3m 11s\tremaining: 28m 41s\n",
      "10:\tlearn: 0.1687829\ttotal: 3m 30s\tremaining: 28m 23s\n",
      "11:\tlearn: 0.1667784\ttotal: 3m 49s\tremaining: 28m 5s\n",
      "12:\tlearn: 0.1637667\ttotal: 4m 9s\tremaining: 27m 47s\n",
      "13:\tlearn: 0.1612539\ttotal: 4m 28s\tremaining: 27m 28s\n",
      "14:\tlearn: 0.1591714\ttotal: 4m 47s\tremaining: 27m 9s\n",
      "15:\tlearn: 0.1578823\ttotal: 5m 6s\tremaining: 26m 50s\n",
      "16:\tlearn: 0.1556772\ttotal: 5m 26s\tremaining: 26m 32s\n",
      "17:\tlearn: 0.1536626\ttotal: 5m 45s\tremaining: 26m 13s\n",
      "18:\tlearn: 0.1517668\ttotal: 6m 4s\tremaining: 25m 54s\n",
      "19:\tlearn: 0.1504214\ttotal: 6m 23s\tremaining: 25m 35s\n",
      "20:\tlearn: 0.1488874\ttotal: 6m 43s\tremaining: 25m 16s\n",
      "21:\tlearn: 0.1470316\ttotal: 7m 2s\tremaining: 24m 57s\n",
      "22:\tlearn: 0.1454244\ttotal: 7m 21s\tremaining: 24m 38s\n",
      "23:\tlearn: 0.1439301\ttotal: 7m 40s\tremaining: 24m 19s\n",
      "24:\tlearn: 0.1427403\ttotal: 7m 59s\tremaining: 23m 59s\n",
      "25:\tlearn: 0.1411995\ttotal: 8m 19s\tremaining: 23m 41s\n",
      "26:\tlearn: 0.1400358\ttotal: 8m 38s\tremaining: 23m 21s\n",
      "27:\tlearn: 0.1381539\ttotal: 8m 57s\tremaining: 23m 2s\n",
      "28:\tlearn: 0.1368083\ttotal: 9m 17s\tremaining: 22m 43s\n",
      "29:\tlearn: 0.1360161\ttotal: 9m 36s\tremaining: 22m 24s\n",
      "30:\tlearn: 0.1350784\ttotal: 9m 55s\tremaining: 22m 5s\n",
      "31:\tlearn: 0.1340258\ttotal: 10m 14s\tremaining: 21m 46s\n",
      "32:\tlearn: 0.1329852\ttotal: 10m 33s\tremaining: 21m 27s\n",
      "33:\tlearn: 0.1323055\ttotal: 10m 53s\tremaining: 21m 7s\n",
      "34:\tlearn: 0.1309161\ttotal: 11m 12s\tremaining: 20m 48s\n",
      "35:\tlearn: 0.1300325\ttotal: 11m 31s\tremaining: 20m 29s\n",
      "36:\tlearn: 0.1292892\ttotal: 11m 50s\tremaining: 20m 10s\n",
      "37:\tlearn: 0.1287064\ttotal: 12m 10s\tremaining: 19m 51s\n",
      "38:\tlearn: 0.1277465\ttotal: 12m 29s\tremaining: 19m 31s\n",
      "39:\tlearn: 0.1271986\ttotal: 12m 48s\tremaining: 19m 12s\n",
      "40:\tlearn: 0.1266559\ttotal: 13m 7s\tremaining: 18m 53s\n",
      "41:\tlearn: 0.1258040\ttotal: 13m 26s\tremaining: 18m 34s\n",
      "42:\tlearn: 0.1253067\ttotal: 13m 46s\tremaining: 18m 15s\n",
      "43:\tlearn: 0.1247922\ttotal: 14m 6s\tremaining: 17m 56s\n",
      "44:\tlearn: 0.1242907\ttotal: 14m 25s\tremaining: 17m 37s\n",
      "45:\tlearn: 0.1238092\ttotal: 14m 44s\tremaining: 17m 18s\n",
      "46:\tlearn: 0.1228298\ttotal: 15m 3s\tremaining: 16m 59s\n",
      "47:\tlearn: 0.1221937\ttotal: 15m 23s\tremaining: 16m 40s\n",
      "48:\tlearn: 0.1217420\ttotal: 15m 42s\tremaining: 16m 20s\n",
      "49:\tlearn: 0.1213376\ttotal: 16m 1s\tremaining: 16m 1s\n",
      "50:\tlearn: 0.1208259\ttotal: 16m 20s\tremaining: 15m 42s\n",
      "51:\tlearn: 0.1198402\ttotal: 16m 40s\tremaining: 15m 23s\n",
      "52:\tlearn: 0.1192389\ttotal: 16m 59s\tremaining: 15m 3s\n",
      "53:\tlearn: 0.1184885\ttotal: 17m 18s\tremaining: 14m 44s\n",
      "54:\tlearn: 0.1180871\ttotal: 17m 38s\tremaining: 14m 25s\n",
      "55:\tlearn: 0.1172417\ttotal: 17m 57s\tremaining: 14m 6s\n",
      "56:\tlearn: 0.1164210\ttotal: 18m 16s\tremaining: 13m 47s\n",
      "57:\tlearn: 0.1160576\ttotal: 18m 35s\tremaining: 13m 28s\n",
      "58:\tlearn: 0.1156329\ttotal: 18m 55s\tremaining: 13m 8s\n",
      "59:\tlearn: 0.1152563\ttotal: 19m 14s\tremaining: 12m 49s\n",
      "60:\tlearn: 0.1145715\ttotal: 19m 33s\tremaining: 12m 30s\n",
      "61:\tlearn: 0.1142074\ttotal: 19m 53s\tremaining: 12m 11s\n",
      "62:\tlearn: 0.1138541\ttotal: 20m 12s\tremaining: 11m 52s\n",
      "63:\tlearn: 0.1132376\ttotal: 20m 32s\tremaining: 11m 33s\n",
      "64:\tlearn: 0.1128819\ttotal: 20m 51s\tremaining: 11m 13s\n",
      "65:\tlearn: 0.1124831\ttotal: 21m 10s\tremaining: 10m 54s\n",
      "66:\tlearn: 0.1121373\ttotal: 21m 30s\tremaining: 10m 35s\n",
      "67:\tlearn: 0.1118099\ttotal: 21m 49s\tremaining: 10m 16s\n",
      "68:\tlearn: 0.1114836\ttotal: 22m 8s\tremaining: 9m 57s\n",
      "69:\tlearn: 0.1111797\ttotal: 22m 28s\tremaining: 9m 37s\n",
      "70:\tlearn: 0.1107785\ttotal: 22m 47s\tremaining: 9m 18s\n",
      "71:\tlearn: 0.1104868\ttotal: 23m 6s\tremaining: 8m 59s\n",
      "72:\tlearn: 0.1101714\ttotal: 23m 26s\tremaining: 8m 40s\n",
      "73:\tlearn: 0.1098987\ttotal: 23m 45s\tremaining: 8m 20s\n",
      "74:\tlearn: 0.1092164\ttotal: 24m 4s\tremaining: 8m 1s\n",
      "75:\tlearn: 0.1089286\ttotal: 24m 24s\tremaining: 7m 42s\n",
      "76:\tlearn: 0.1086562\ttotal: 24m 43s\tremaining: 7m 23s\n",
      "77:\tlearn: 0.1083943\ttotal: 25m 2s\tremaining: 7m 3s\n",
      "78:\tlearn: 0.1081211\ttotal: 25m 22s\tremaining: 6m 44s\n",
      "79:\tlearn: 0.1078639\ttotal: 25m 42s\tremaining: 6m 25s\n",
      "80:\tlearn: 0.1076042\ttotal: 26m 3s\tremaining: 6m 6s\n",
      "81:\tlearn: 0.1073538\ttotal: 26m 25s\tremaining: 5m 47s\n",
      "82:\tlearn: 0.1071141\ttotal: 26m 45s\tremaining: 5m 28s\n",
      "83:\tlearn: 0.1065938\ttotal: 27m 4s\tremaining: 5m 9s\n",
      "84:\tlearn: 0.1061754\ttotal: 27m 24s\tremaining: 4m 50s\n",
      "85:\tlearn: 0.1053194\ttotal: 27m 45s\tremaining: 4m 31s\n",
      "86:\tlearn: 0.1050808\ttotal: 28m 5s\tremaining: 4m 11s\n",
      "87:\tlearn: 0.1048400\ttotal: 28m 25s\tremaining: 3m 52s\n",
      "88:\tlearn: 0.1045152\ttotal: 28m 46s\tremaining: 3m 33s\n",
      "89:\tlearn: 0.1042809\ttotal: 29m 6s\tremaining: 3m 14s\n",
      "90:\tlearn: 0.1040585\ttotal: 29m 26s\tremaining: 2m 54s\n",
      "91:\tlearn: 0.1037957\ttotal: 29m 45s\tremaining: 2m 35s\n",
      "92:\tlearn: 0.1035700\ttotal: 30m 5s\tremaining: 2m 15s\n",
      "93:\tlearn: 0.1033645\ttotal: 30m 25s\tremaining: 1m 56s\n",
      "94:\tlearn: 0.1030949\ttotal: 30m 44s\tremaining: 1m 37s\n",
      "95:\tlearn: 0.1027606\ttotal: 31m 3s\tremaining: 1m 17s\n",
      "96:\tlearn: 0.1021871\ttotal: 31m 23s\tremaining: 58.2s\n",
      "97:\tlearn: 0.1019758\ttotal: 31m 42s\tremaining: 38.8s\n",
      "98:\tlearn: 0.1015395\ttotal: 32m 2s\tremaining: 19.4s\n",
      "99:\tlearn: 0.1013031\ttotal: 32m 21s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7fc39b7a7f70>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_text, train['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7532690984170681"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test['toxic'], model.predict(X_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_LGBM = lgbm.LGBMClassifier(verbose=-1, learning_rate=0.5, max_depth=20, num_leaves=50, n_estimators=120, max_bin=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.5, max_bin=2000, max_depth=20, n_estimators=120,\n",
       "               num_leaves=50, verbose=-1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_LGBM.fit(X_train_text, train['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7664162038549494"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test['toxic'], clf_LGBM.predict(X_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И так приступим, для начала я удаляю все переменные использованные ранее чтобы наше ядро не крашнулось, и памяти хватило. Затем токенизирую все текста(ну почти все).<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import transformers \n",
    "import numpy as np\n",
    "from tqdm import notebook\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(\n",
    "  clean_text=False,\n",
    "  handle_chinese_chars=False,\n",
    "  strip_accents=False,\n",
    "  lowercase=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased', model_max_len=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = df['text'].apply(\n",
    "  lambda x: tokenizer.encode(x, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как длина в нашей макс модели 512 то нам нужно ограничить наш диапазон на 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "for i in range(len(tokenized)):\n",
    "    if len(tokenized.iloc[i]) > 512:\n",
    "        index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized.drop(index=index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [101, 16409, 1643, 20592, 2116, 2009, 1103, 14...\n",
       "1         [101, 141, 112, 170, 2246, 2246, 106, 1124, 26...\n",
       "2         [101, 4403, 1299, 117, 146, 112, 182, 1541, 11...\n",
       "3         [101, 107, 3046, 146, 1169, 112, 189, 1294, 12...\n",
       "4         [101, 1192, 117, 6442, 117, 1132, 1139, 6485, ...\n",
       "                                ...                        \n",
       "159566    [101, 107, 131, 131, 131, 131, 131, 1262, 1111...\n",
       "159567    [101, 1192, 1431, 1129, 16155, 1104, 3739, 133...\n",
       "159568    [101, 156, 18965, 6198, 12189, 1306, 117, 1175...\n",
       "159569    [101, 1262, 1122, 2736, 1176, 1122, 1108, 2140...\n",
       "159570    [101, 107, 1262, 119, 119, 119, 146, 1541, 127...\n",
       "Name: text, Length: 155749, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом не так уж и много данных ушло<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f911d03df4f4837be885a675cbc0707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 42min 55s, sys: 1.41 s, total: 1h 42min 56s\n",
      "Wall time: 1h 42min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import notebook\n",
    "batch_size = 100 # для примера возьмем такой батч, где будет всего две строки датасета\n",
    "embeddings = [] \n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]).to(device) # закидываем тензор на GPU\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.to(device)\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy()) # перевод обратно на проц, чтобы в нумпай кинуть\n",
    "        del batch\n",
    "        del attention_mask_batch\n",
    "        del batch_embeddings\n",
    "        \n",
    "features = np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155700, 768)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(index=index).iloc[:155700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159517</th>\n",
       "      <td>\"\\nYou've got no call to start attacking me ad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159518</th>\n",
       "      <td>Agreed, besides debating it now won't repair d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159519</th>\n",
       "      <td>REDIRECT Talk:Harry Yates (footballer, born 1925)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159520</th>\n",
       "      <td>HEY HOW ABOUT SHE PUTS OUT A NEW SINGLE AM I R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159521</th>\n",
       "      <td>Improv\\nWhat is the name of the part where the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155700 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "0       Explanation\\nWhy the edits made under my usern...      0\n",
       "1       D'aww! He matches this background colour I'm s...      0\n",
       "2       Hey man, I'm really not trying to edit war. It...      0\n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4       You, sir, are my hero. Any chance you remember...      0\n",
       "...                                                   ...    ...\n",
       "159517  \"\\nYou've got no call to start attacking me ad...      0\n",
       "159518  Agreed, besides debating it now won't repair d...      0\n",
       "159519  REDIRECT Talk:Harry Yates (footballer, born 1925)      0\n",
       "159520  HEY HOW ABOUT SHE PUTS OUT A NEW SINGLE AM I R...      0\n",
       "159521  Improv\\nWhat is the name of the part where the...      0\n",
       "\n",
       "[155700 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5411031 ,  0.01921145,  0.11053315, ..., -0.3546025 ,\n",
       "         0.3625402 ,  0.0970161 ],\n",
       "       [ 0.3427579 , -0.10840041,  0.31300983, ...,  0.2157756 ,\n",
       "        -0.01797534,  0.13064337],\n",
       "       [ 0.56802046,  0.23963812, -0.15950912, ..., -0.28971767,\n",
       "         0.16949017, -0.04347336],\n",
       "       ...,\n",
       "       [ 0.22448054,  0.03906267, -0.09136067, ..., -0.5576799 ,\n",
       "         0.1713735 ,  0.5461098 ],\n",
       "       [ 0.58344555, -0.10617191,  0.1454983 , ..., -0.18654124,\n",
       "         0.44484606,  0.42675206],\n",
       "       [ 0.38280675,  0.10741405, -0.02836372, ..., -0.18946797,\n",
       "        -0.04672127, -0.01444094]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:124560]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127622</th>\n",
       "      <td>\"\\n\\nHi - I started editing in January of this...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127623</th>\n",
       "      <td>Thanks for your note \\n\\nI have been dealing w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127624</th>\n",
       "      <td>\"\\n\\n This person is noteable \\nNotability of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127625</th>\n",
       "      <td>Spam \\nQuit inserting linkspam into the Trinid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127626</th>\n",
       "      <td>\"\\nPardon me for butting in, DGG, but Stifle d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124560 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "0       Explanation\\nWhy the edits made under my usern...      0\n",
       "1       D'aww! He matches this background colour I'm s...      0\n",
       "2       Hey man, I'm really not trying to edit war. It...      0\n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4       You, sir, are my hero. Any chance you remember...      0\n",
       "...                                                   ...    ...\n",
       "127622  \"\\n\\nHi - I started editing in January of this...      0\n",
       "127623  Thanks for your note \\n\\nI have been dealing w...      0\n",
       "127624  \"\\n\\n This person is noteable \\nNotability of ...      0\n",
       "127625  Spam \\nQuit inserting linkspam into the Trinid...      0\n",
       "127626  \"\\nPardon me for butting in, DGG, but Stifle d...      0\n",
       "\n",
       "[124560 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:124560]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После embedding'a настало время научить модель на одной из моделей, например LogRegression<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=50.0, max_iter=124560, multi_class='multinomial',\n",
       "                   n_jobs=-1, random_state=17)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model = LogisticRegression(max_iter=124560, C=5e1, solver='lbfgs', multi_class='multinomial', random_state=17, n_jobs=-1)\n",
    "log_model.fit(features[:124560], df.iloc[:124560]['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31140, 768)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[124560:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155700, 768)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model.predict(features[124560:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7124113475177306"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df[124560:]['toxic'], log_model.predict(features[124560:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.785066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.653307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostClassifier</th>\n",
       "      <td>0.753269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.766416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT</th>\n",
       "      <td>0.712411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    f1_score\n",
       "LogisticRegression  0.785066\n",
       "SGDClassifier       0.653307\n",
       "CatBoostClassifier  0.753269\n",
       "LGBMClassifier      0.766416\n",
       "BERT                0.712411"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([0.7850659359479363,0.6533066132264529,0.7532690984170681,0.7664162038549494, 0.712411347517730], columns=['f1_score'], \n",
    "             index=['LogisticRegression','SGDClassifier','CatBoostClassifier','LGBMClassifier', 'BERT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученные результаты свидетельствут о том что в целом все модели предсказывают достаточно неплохо, но в рамках данного проекта нужно преодолеть значение f1 > 0.75 с чем справились все, за исключением стохастического градиентого спуска. А также BERT не достиг значения 0,75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 821,
    "start_time": "2021-09-02T21:04:16.317Z"
   },
   {
    "duration": 1815,
    "start_time": "2021-09-02T21:04:37.478Z"
   },
   {
    "duration": 31,
    "start_time": "2021-09-02T21:04:39.297Z"
   },
   {
    "duration": 1763,
    "start_time": "2021-09-03T18:48:21.414Z"
   },
   {
    "duration": 82,
    "start_time": "2021-09-03T18:48:40.143Z"
   },
   {
    "duration": 46,
    "start_time": "2021-09-03T18:49:08.011Z"
   },
   {
    "duration": 12,
    "start_time": "2021-09-03T18:49:12.511Z"
   },
   {
    "duration": 3,
    "start_time": "2021-09-03T18:49:26.207Z"
   },
   {
    "duration": 4,
    "start_time": "2021-09-03T18:51:32.029Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
